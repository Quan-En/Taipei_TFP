{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/r26094022/NCKU_Social_Network/final_project/ASTGCN-r-pytorch-master'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/PEMS08/PEMS08.npz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_signal_matrix_filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Section: Data>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prepareData\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_data(sequence_length, num_of_depend, label_start_idx,\n",
    "                num_for_predict, units, points_per_hour):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_length: int, length of all history data\n",
    "    num_of_depend: int,\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int, the number of points will be predicted for each sample\n",
    "    units: int, week: 7 * 24, day: 24, recent(hour): 1\n",
    "    points_per_hour: int, number of points per hour, depends on data\n",
    "    Returns\n",
    "    ----------\n",
    "    list[(start_idx, end_idx)]\n",
    "    '''\n",
    "\n",
    "    if points_per_hour < 0:\n",
    "        raise ValueError(\"points_per_hour should be greater than 0!\")\n",
    "\n",
    "    if label_start_idx + num_for_predict > sequence_length:\n",
    "        return None\n",
    "\n",
    "    x_idx = []\n",
    "    for i in range(1, num_of_depend + 1):\n",
    "        start_idx = label_start_idx - points_per_hour * units * i\n",
    "        end_idx = start_idx + num_for_predict\n",
    "        if start_idx >= 0:\n",
    "            x_idx.append((start_idx, end_idx))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if len(x_idx) != num_of_depend:\n",
    "        return None\n",
    "\n",
    "    return x_idx[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_indices(data_sequence, num_of_weeks, num_of_days, num_of_hours,\n",
    "                       label_start_idx, num_for_predict, points_per_hour=12):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_sequence: np.ndarray\n",
    "                   shape is (sequence_length, num_of_vertices, num_of_features)\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    label_start_idx: int, the first index of predicting target, 预测值开始的那个点\n",
    "    num_for_predict: int,\n",
    "                     the number of points will be predicted for each sample\n",
    "    points_per_hour: int, default 12, number of points per hour\n",
    "    Returns\n",
    "    ----------\n",
    "    week_sample: np.ndarray\n",
    "                 shape is (num_of_weeks * points_per_hour,\n",
    "                           num_of_vertices, num_of_features)\n",
    "    day_sample: np.ndarray\n",
    "                 shape is (num_of_days * points_per_hour,\n",
    "                           num_of_vertices, num_of_features)\n",
    "    hour_sample: np.ndarray\n",
    "                 shape is (num_of_hours * points_per_hour,\n",
    "                           num_of_vertices, num_of_features)\n",
    "    target: np.ndarray\n",
    "            shape is (num_for_predict, num_of_vertices, num_of_features)\n",
    "    '''\n",
    "    week_sample, day_sample, hour_sample = None, None, None\n",
    "\n",
    "    if label_start_idx + num_for_predict > data_sequence.shape[0]:\n",
    "        return week_sample, day_sample, hour_sample, None\n",
    "\n",
    "    if num_of_weeks > 0:\n",
    "        week_indices = search_data(data_sequence.shape[0], num_of_weeks,\n",
    "                                   label_start_idx, num_for_predict,\n",
    "                                   7 * 24, points_per_hour)\n",
    "        if not week_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        week_sample = np.concatenate([data_sequence[i: j]\n",
    "                                      for i, j in week_indices], axis=0)\n",
    "\n",
    "    if num_of_days > 0:\n",
    "        day_indices = search_data(data_sequence.shape[0], num_of_days,\n",
    "                                  label_start_idx, num_for_predict,\n",
    "                                  24, points_per_hour)\n",
    "        if not day_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        day_sample = np.concatenate([data_sequence[i: j]\n",
    "                                     for i, j in day_indices], axis=0)\n",
    "\n",
    "    if num_of_hours > 0:\n",
    "        hour_indices = search_data(data_sequence.shape[0], num_of_hours,\n",
    "                                   label_start_idx, num_for_predict,\n",
    "                                   1, points_per_hour)\n",
    "        if not hour_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        hour_sample = np.concatenate([data_sequence[i: j]\n",
    "                                      for i, j in hour_indices], axis=0)\n",
    "\n",
    "    target = data_sequence[label_start_idx: label_start_idx + num_for_predict]\n",
    "\n",
    "    return week_sample, day_sample, hour_sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_generate_dataset(graph_signal_matrix_filename,\n",
    "                                                     num_of_weeks, num_of_days,\n",
    "                                                     num_of_hours, num_for_predict,\n",
    "                                                     points_per_hour=12, save=False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_signal_matrix_filename: str, path of graph signal matrix file\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    num_for_predict: int\n",
    "    points_per_hour: int, default 12, depends on data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    feature: np.ndarray,\n",
    "             shape is (num_of_samples, num_of_depend * points_per_hour,\n",
    "                       num_of_vertices, num_of_features)\n",
    "    target: np.ndarray,\n",
    "            shape is (num_of_samples, num_of_vertices, num_for_predict)\n",
    "    '''\n",
    "    data_seq = np.load(graph_signal_matrix_filename)['data']  # (sequence_length, num_of_vertices, num_of_features)\n",
    "\n",
    "    all_samples = []\n",
    "    for idx in range(data_seq.shape[0]):\n",
    "        sample = get_sample_indices(data_seq, num_of_weeks, num_of_days,\n",
    "                                    num_of_hours, idx, num_for_predict,\n",
    "                                    points_per_hour)\n",
    "        if ((sample[0] is None) and (sample[1] is None) and (sample[2] is None)):\n",
    "            continue\n",
    "\n",
    "        week_sample, day_sample, hour_sample, target = sample\n",
    "\n",
    "        sample = []  # [(week_sample),(day_sample),(hour_sample),target,time_sample]\n",
    "\n",
    "        if num_of_weeks > 0:\n",
    "            week_sample = np.expand_dims(week_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(week_sample)\n",
    "\n",
    "        if num_of_days > 0:\n",
    "            day_sample = np.expand_dims(day_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(day_sample)\n",
    "\n",
    "        if num_of_hours > 0:\n",
    "            hour_sample = np.expand_dims(hour_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(hour_sample)\n",
    "\n",
    "        target = np.expand_dims(target, axis=0).transpose((0, 2, 3, 1))[:, :, 0, :]  # (1,N,T)\n",
    "        sample.append(target)\n",
    "\n",
    "        time_sample = np.expand_dims(np.array([idx]), axis=0)  # (1,1)\n",
    "        sample.append(time_sample)\n",
    "\n",
    "        all_samples.append(\n",
    "            sample)  # sampe：[(week_sample),(day_sample),(hour_sample),target,time_sample] = [(1,N,F,Tw),(1,N,F,Td),(1,N,F,Th),(1,N,Tpre),(1,1)]\n",
    "\n",
    "    split_line1 = int(len(all_samples) * 0.6)\n",
    "    split_line2 = int(len(all_samples) * 0.8)\n",
    "\n",
    "    training_set = [np.concatenate(i, axis=0)\n",
    "                    for i in zip(*all_samples[:split_line1])]  # [(B,N,F,Tw),(B,N,F,Td),(B,N,F,Th),(B,N,Tpre),(B,1)]\n",
    "    validation_set = [np.concatenate(i, axis=0)\n",
    "                      for i in zip(*all_samples[split_line1: split_line2])]\n",
    "    testing_set = [np.concatenate(i, axis=0)\n",
    "                   for i in zip(*all_samples[split_line2:])]\n",
    "\n",
    "    train_x = np.concatenate(training_set[:-2], axis=-1)  # (B,N,F,T')\n",
    "    val_x = np.concatenate(validation_set[:-2], axis=-1)\n",
    "    test_x = np.concatenate(testing_set[:-2], axis=-1)\n",
    "\n",
    "    train_target = training_set[-2]  # (B,N,T)\n",
    "    val_target = validation_set[-2]\n",
    "    test_target = testing_set[-2]\n",
    "\n",
    "    train_timestamp = training_set[-1]  # (B,1)\n",
    "    val_timestamp = validation_set[-1]\n",
    "    test_timestamp = testing_set[-1]\n",
    "\n",
    "    (stats, train_x_norm, val_x_norm, test_x_norm) = normalization(train_x, val_x, test_x)\n",
    "\n",
    "    all_data = {\n",
    "        'train': {\n",
    "            'x': train_x_norm,\n",
    "            'target': train_target,\n",
    "            'timestamp': train_timestamp,\n",
    "        },\n",
    "        'val': {\n",
    "            'x': val_x_norm,\n",
    "            'target': val_target,\n",
    "            'timestamp': val_timestamp,\n",
    "        },\n",
    "        'test': {\n",
    "            'x': test_x_norm,\n",
    "            'target': test_target,\n",
    "            'timestamp': test_timestamp,\n",
    "        },\n",
    "        'stats': {\n",
    "            '_mean': stats['_mean'],\n",
    "            '_std': stats['_std'],\n",
    "        }\n",
    "    }\n",
    "    print('train x:', all_data['train']['x'].shape)\n",
    "    print('train target:', all_data['train']['target'].shape)\n",
    "    print('train timestamp:', all_data['train']['timestamp'].shape)\n",
    "    print()\n",
    "    print('val x:', all_data['val']['x'].shape)\n",
    "    print('val target:', all_data['val']['target'].shape)\n",
    "    print('val timestamp:', all_data['val']['timestamp'].shape)\n",
    "    print()\n",
    "    print('test x:', all_data['test']['x'].shape)\n",
    "    print('test target:', all_data['test']['target'].shape)\n",
    "    print('test timestamp:', all_data['test']['timestamp'].shape)\n",
    "    print()\n",
    "    print('train data _mean :', stats['_mean'].shape, stats['_mean'])\n",
    "    print('train data _std :', stats['_std'].shape, stats['_std'])\n",
    "\n",
    "    if save:\n",
    "        file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "        dirpath = os.path.dirname(graph_signal_matrix_filename)\n",
    "        filename = os.path.join(dirpath, file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) + '_astcgn'\n",
    "        print('save file:', filename)\n",
    "        np.savez_compressed(filename,\n",
    "                            train_x=all_data['train']['x'], train_target=all_data['train']['target'],\n",
    "                            train_timestamp=all_data['train']['timestamp'],\n",
    "                            val_x=all_data['val']['x'], val_target=all_data['val']['target'],\n",
    "                            val_timestamp=all_data['val']['timestamp'],\n",
    "                            test_x=all_data['test']['x'], test_target=all_data['test']['target'],\n",
    "                            test_timestamp=all_data['test']['timestamp'],\n",
    "                            mean=all_data['stats']['_mean'], std=all_data['stats']['_std']\n",
    "                            )\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train, val, test):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train, val, test: np.ndarray (B,N,F,T)\n",
    "    Returns\n",
    "    ----------\n",
    "    stats: dict, two keys: mean and std\n",
    "    train_norm, val_norm, test_norm: np.ndarray,\n",
    "                                     shape is the same as original\n",
    "    '''\n",
    "\n",
    "    assert train.shape[1:] == val.shape[1:] and val.shape[1:] == test.shape[1:]  # ensure the num of nodes is the same\n",
    "    mean = train.mean(axis=(0,1,3), keepdims=True)\n",
    "    std = train.std(axis=(0,1,3), keepdims=True)\n",
    "    print('mean.shape:',mean.shape)\n",
    "    print('std.shape:',std.shape)\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "\n",
    "    train_norm = normalize(train)\n",
    "    val_norm = normalize(val)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return {'_mean': mean, '_std': std}, train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read configuration file: configurations/PEMS08_astgcn.conf\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", default='configurations/PEMS08_astgcn.conf', type=str,\n",
    "                    help=\"configuration file path\")\n",
    "args = parser.parse_args(args=[])\n",
    "config = configparser.ConfigParser()\n",
    "print('Read configuration file: %s' % (args.config))\n",
    "config.read(args.config)\n",
    "data_config = config['Data']\n",
    "training_config = config['Training']\n",
    "\n",
    "adj_filename = data_config['adj_filename']\n",
    "graph_signal_matrix_filename = data_config['graph_signal_matrix_filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.has_option('Data', 'id_filename'):\n",
    "    id_filename = data_config['id_filename']\n",
    "else:\n",
    "    id_filename = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(id_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_vertices = int(data_config['num_of_vertices'])\n",
    "points_per_hour = int(data_config['points_per_hour'])\n",
    "num_for_predict = int(data_config['num_for_predict'])\n",
    "len_input = int(data_config['len_input'])\n",
    "dataset_name = data_config['dataset_name']\n",
    "num_of_weeks = int(training_config['num_of_weeks'])\n",
    "num_of_days = int(training_config['num_of_days'])\n",
    "num_of_hours = int(training_config['num_of_hours'])\n",
    "num_of_vertices = int(data_config['num_of_vertices'])\n",
    "points_per_hour = int(data_config['points_per_hour'])\n",
    "num_for_predict = int(data_config['num_for_predict'])\n",
    "graph_signal_matrix_filename = data_config['graph_signal_matrix_filename']\n",
    "graph_signal_matrix_filename = './data/PEMS08/PEMS08.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_weeks, num_of_days, num_of_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_input,num_for_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_hours, num_for_predict, points_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(graph_signal_matrix_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17856, 170, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean.shape: (1, 1, 3, 1)\n",
      "std.shape: (1, 1, 3, 1)\n",
      "train x: (10699, 170, 3, 12)\n",
      "train target: (10699, 170, 12)\n",
      "train timestamp: (10699, 1)\n",
      "\n",
      "val x: (3567, 170, 3, 12)\n",
      "val target: (3567, 170, 12)\n",
      "val timestamp: (3567, 1)\n",
      "\n",
      "test x: (3567, 170, 3, 12)\n",
      "test target: (3567, 170, 12)\n",
      "test timestamp: (3567, 1)\n",
      "\n",
      "train data _mean : (1, 1, 3, 1) [[[[2.29858934e+02]\n",
      "   [6.43476941e-02]\n",
      "   [6.37841954e+01]]]]\n",
      "train data _std : (1, 1, 3, 1) [[[[1.45622681e+02]\n",
      "   [4.46275336e-02]\n",
      "   [6.53075967e+00]]]]\n",
      "save file: ./data/PEMS08/PEMS08_r1_d0_w0_astcgn\n"
     ]
    }
   ],
   "source": [
    "all_data = read_and_generate_dataset(graph_signal_matrix_filename, num_of_weeks, num_of_days, num_of_hours, num_for_predict, points_per_hour=points_per_hour, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10699, 170, 3, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['train']['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import scaled_Laplacian, cheb_polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Spatial_Attention_layer(nn.Module):\n",
    "    '''\n",
    "    compute spatial attention scores\n",
    "    '''\n",
    "    def __init__(self, DEVICE, in_channels, num_of_vertices, num_of_timesteps):\n",
    "        super(Spatial_Attention_layer, self).__init__()\n",
    "        self.W1 = nn.Parameter(torch.FloatTensor(num_of_timesteps).to(DEVICE))\n",
    "        self.W2 = nn.Parameter(torch.FloatTensor(in_channels, num_of_timesteps).to(DEVICE))\n",
    "        self.W3 = nn.Parameter(torch.FloatTensor(in_channels).to(DEVICE))\n",
    "        self.bs = nn.Parameter(torch.FloatTensor(1, num_of_vertices, num_of_vertices).to(DEVICE))\n",
    "        self.Vs = nn.Parameter(torch.FloatTensor(num_of_vertices, num_of_vertices).to(DEVICE))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (batch_size, N, F_in, T)\n",
    "        :return: (B,N,N)\n",
    "        '''\n",
    "\n",
    "        lhs = torch.matmul(torch.matmul(x, self.W1), self.W2)  # (b,N,F,T)(T)->(b,N,F)(F,T)->(b,N,T)\n",
    "\n",
    "        rhs = torch.matmul(self.W3, x).transpose(-1, -2)  # (F)(b,N,F,T)->(b,N,T)->(b,T,N)\n",
    "\n",
    "        product = torch.matmul(lhs, rhs)  # (b,N,T)(b,T,N) -> (B, N, N)\n",
    "\n",
    "        S = torch.matmul(self.Vs, torch.sigmoid(product + self.bs))  # (N,N)(B, N, N)->(B,N,N)\n",
    "\n",
    "        S_normalized = F.softmax(S, dim=1)\n",
    "\n",
    "        return S_normalized\n",
    "\n",
    "\n",
    "class cheb_conv_withSAt(nn.Module):\n",
    "    '''\n",
    "    K-order chebyshev graph convolution\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, cheb_polynomials, in_channels, out_channels):\n",
    "        '''\n",
    "        :param K: int\n",
    "        :param in_channles: int, num of channels in the input sequence\n",
    "        :param out_channels: int, num of channels in the output sequence\n",
    "        '''\n",
    "        super(cheb_conv_withSAt, self).__init__()\n",
    "        self.K = K\n",
    "        self.cheb_polynomials = cheb_polynomials\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.DEVICE = cheb_polynomials[0].device\n",
    "        self.Theta = nn.ParameterList([nn.Parameter(torch.FloatTensor(in_channels, out_channels).to(self.DEVICE)) for _ in range(K)])\n",
    "\n",
    "    def forward(self, x, spatial_attention):\n",
    "        '''\n",
    "        Chebyshev graph convolution operation\n",
    "        :param x: (batch_size, N, F_in, T)\n",
    "        :return: (batch_size, N, F_out, T)\n",
    "        '''\n",
    "\n",
    "        batch_size, num_of_vertices, in_channels, num_of_timesteps = x.shape\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for time_step in range(num_of_timesteps):\n",
    "\n",
    "            graph_signal = x[:, :, :, time_step]  # (b, N, F_in)\n",
    "\n",
    "            output = torch.zeros(batch_size, num_of_vertices, self.out_channels).to(self.DEVICE)  # (b, N, F_out)\n",
    "\n",
    "            for k in range(self.K):\n",
    "\n",
    "                T_k = self.cheb_polynomials[k]  # (N,N)\n",
    "\n",
    "                T_k_with_at = T_k.mul(spatial_attention)   # (N,N)*(N,N) = (N,N) 多行和为1, 按着列进行归一化\n",
    "\n",
    "                theta_k = self.Theta[k]  # (in_channel, out_channel)\n",
    "\n",
    "                rhs = T_k_with_at.permute(0, 2, 1).matmul(graph_signal)  # (N, N)(b, N, F_in) = (b, N, F_in) 因为是左乘，所以多行和为1变为多列和为1，即一行之和为1，进行左乘\n",
    "\n",
    "                output = output + rhs.matmul(theta_k)  # (b, N, F_in)(F_in, F_out) = (b, N, F_out)\n",
    "\n",
    "            outputs.append(output.unsqueeze(-1))  # (b, N, F_out, 1)\n",
    "\n",
    "        return F.relu(torch.cat(outputs, dim=-1))  # (b, N, F_out, T)\n",
    "\n",
    "\n",
    "class Temporal_Attention_layer(nn.Module):\n",
    "    def __init__(self, DEVICE, in_channels, num_of_vertices, num_of_timesteps):\n",
    "        super(Temporal_Attention_layer, self).__init__()\n",
    "        self.U1 = nn.Parameter(torch.FloatTensor(num_of_vertices).to(DEVICE))\n",
    "        self.U2 = nn.Parameter(torch.FloatTensor(in_channels, num_of_vertices).to(DEVICE))\n",
    "        self.U3 = nn.Parameter(torch.FloatTensor(in_channels).to(DEVICE))\n",
    "        self.be = nn.Parameter(torch.FloatTensor(1, num_of_timesteps, num_of_timesteps).to(DEVICE))\n",
    "        self.Ve = nn.Parameter(torch.FloatTensor(num_of_timesteps, num_of_timesteps).to(DEVICE))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (batch_size, N, F_in, T)\n",
    "        :return: (B, T, T)\n",
    "        '''\n",
    "        _, num_of_vertices, num_of_features, num_of_timesteps = x.shape\n",
    "\n",
    "        lhs = torch.matmul(torch.matmul(x.permute(0, 3, 2, 1), self.U1), self.U2)\n",
    "        # x:(B, N, F_in, T) -> (B, T, F_in, N)\n",
    "        # (B, T, F_in, N)(N) -> (B,T,F_in)\n",
    "        # (B,T,F_in)(F_in,N)->(B,T,N)\n",
    "\n",
    "        rhs = torch.matmul(self.U3, x)  # (F)(B,N,F,T)->(B, N, T)\n",
    "\n",
    "        product = torch.matmul(lhs, rhs)  # (B,T,N)(B,N,T)->(B,T,T)\n",
    "\n",
    "        E = torch.matmul(self.Ve, torch.sigmoid(product + self.be))  # (B, T, T)\n",
    "\n",
    "        E_normalized = F.softmax(E, dim=1)\n",
    "\n",
    "        return E_normalized\n",
    "\n",
    "\n",
    "class cheb_conv(nn.Module):\n",
    "    '''\n",
    "    K-order chebyshev graph convolution\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, cheb_polynomials, in_channels, out_channels):\n",
    "        '''\n",
    "        :param K: int\n",
    "        :param in_channles: int, num of channels in the input sequence\n",
    "        :param out_channels: int, num of channels in the output sequence\n",
    "        '''\n",
    "        super(cheb_conv, self).__init__()\n",
    "        self.K = K\n",
    "        self.cheb_polynomials = cheb_polynomials\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.DEVICE = cheb_polynomials[0].device\n",
    "        self.Theta = nn.ParameterList([nn.Parameter(torch.FloatTensor(in_channels, out_channels).to(self.DEVICE)) for _ in range(K)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Chebyshev graph convolution operation\n",
    "        :param x: (batch_size, N, F_in, T)\n",
    "        :return: (batch_size, N, F_out, T)\n",
    "        '''\n",
    "\n",
    "        batch_size, num_of_vertices, in_channels, num_of_timesteps = x.shape\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for time_step in range(num_of_timesteps):\n",
    "\n",
    "            graph_signal = x[:, :, :, time_step]  # (b, N, F_in)\n",
    "\n",
    "            output = torch.zeros(batch_size, num_of_vertices, self.out_channels).to(self.DEVICE)  # (b, N, F_out)\n",
    "\n",
    "            for k in range(self.K):\n",
    "\n",
    "                T_k = self.cheb_polynomials[k]  # (N,N)\n",
    "\n",
    "                theta_k = self.Theta[k]  # (in_channel, out_channel)\n",
    "\n",
    "                rhs = graph_signal.permute(0, 2, 1).matmul(T_k).permute(0, 2, 1)\n",
    "\n",
    "                output = output + rhs.matmul(theta_k)\n",
    "\n",
    "            outputs.append(output.unsqueeze(-1))\n",
    "\n",
    "        return F.relu(torch.cat(outputs, dim=-1))\n",
    "\n",
    "\n",
    "class ASTGCN_block(nn.Module):\n",
    "\n",
    "    def __init__(self, DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_of_vertices, num_of_timesteps):\n",
    "        super(ASTGCN_block, self).__init__()\n",
    "        self.TAt = Temporal_Attention_layer(DEVICE, in_channels, num_of_vertices, num_of_timesteps)\n",
    "        self.SAt = Spatial_Attention_layer(DEVICE, in_channels, num_of_vertices, num_of_timesteps)\n",
    "        self.cheb_conv_SAt = cheb_conv_withSAt(K, cheb_polynomials, in_channels, nb_chev_filter)\n",
    "        self.time_conv = nn.Conv2d(nb_chev_filter, nb_time_filter, kernel_size=(1, 3), stride=(1, time_strides), padding=(0, 1))\n",
    "        self.residual_conv = nn.Conv2d(in_channels, nb_time_filter, kernel_size=(1, 1), stride=(1, time_strides))\n",
    "        self.ln = nn.LayerNorm(nb_time_filter)  #需要将channel放到最后一个维度上\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (batch_size, N, F_in, T)\n",
    "        :return: (batch_size, N, nb_time_filter, T)\n",
    "        '''\n",
    "        batch_size, num_of_vertices, num_of_features, num_of_timesteps = x.shape\n",
    "\n",
    "        # TAt\n",
    "        temporal_At = self.TAt(x)  # (b, T, T)\n",
    "\n",
    "        x_TAt = torch.matmul(x.reshape(batch_size, -1, num_of_timesteps), temporal_At).reshape(batch_size, num_of_vertices, num_of_features, num_of_timesteps)\n",
    "\n",
    "        # SAt\n",
    "        spatial_At = self.SAt(x_TAt)\n",
    "\n",
    "        # cheb gcn\n",
    "        spatial_gcn = self.cheb_conv_SAt(x, spatial_At)  # (b,N,F,T)\n",
    "        # spatial_gcn = self.cheb_conv(x)\n",
    "\n",
    "        # convolution along the time axis\n",
    "        time_conv_output = self.time_conv(spatial_gcn.permute(0, 2, 1, 3))  # (b,N,F,T)->(b,F,N,T) 用(1,3)的卷积核去做->(b,F,N,T)\n",
    "\n",
    "        # residual shortcut\n",
    "        x_residual = self.residual_conv(x.permute(0, 2, 1, 3))  # (b,N,F,T)->(b,F,N,T) 用(1,1)的卷积核去做->(b,F,N,T)\n",
    "\n",
    "        x_residual = self.ln(F.relu(x_residual + time_conv_output).permute(0, 3, 2, 1)).permute(0, 2, 3, 1)\n",
    "        # (b,F,N,T)->(b,T,N,F) -ln-> (b,T,N,F)->(b,N,F,T)\n",
    "\n",
    "        return x_residual\n",
    "\n",
    "\n",
    "class ASTGCN_submodule(nn.Module):\n",
    "\n",
    "    def __init__(self, DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices):\n",
    "        '''\n",
    "        :param nb_block:\n",
    "        :param in_channels:\n",
    "        :param K:\n",
    "        :param nb_chev_filter:\n",
    "        :param nb_time_filter:\n",
    "        :param time_strides:\n",
    "        :param cheb_polynomials:\n",
    "        :param nb_predict_step:\n",
    "        '''\n",
    "\n",
    "        super(ASTGCN_submodule, self).__init__()\n",
    "\n",
    "        self.BlockList = nn.ModuleList([ASTGCN_block(DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_of_vertices, len_input)])\n",
    "\n",
    "        self.BlockList.extend([ASTGCN_block(DEVICE, nb_time_filter, K, nb_chev_filter, nb_time_filter, 1, cheb_polynomials, num_of_vertices, len_input//time_strides) for _ in range(nb_block-1)])\n",
    "\n",
    "        self.final_conv = nn.Conv2d(int(len_input/time_strides), num_for_predict, kernel_size=(1, nb_time_filter))\n",
    "\n",
    "        self.DEVICE = DEVICE\n",
    "\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (B, N_nodes, F_in, T_in)\n",
    "        :return: (B, N_nodes, T_out)\n",
    "        '''\n",
    "        for block in self.BlockList:\n",
    "            x = block(x)\n",
    "\n",
    "        output = self.final_conv(x.permute(0, 3, 1, 2))[:, :, :, -1].permute(0, 2, 1)\n",
    "        # (b,N,F,T)->(b,T,N,F)-conv<1,F>->(b,c_out*T,N,1)->(b,c_out*T,N)->(b,N,T)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def make_model(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, adj_mx, num_for_predict, len_input, num_of_vertices):\n",
    "    '''\n",
    "\n",
    "    :param DEVICE:\n",
    "    :param nb_block:\n",
    "    :param in_channels:\n",
    "    :param K:\n",
    "    :param nb_chev_filter:\n",
    "    :param nb_time_filter:\n",
    "    :param time_strides:\n",
    "    :param cheb_polynomials:\n",
    "    :param nb_predict_step:\n",
    "    :param len_input\n",
    "    :return:\n",
    "    '''\n",
    "    L_tilde = scaled_Laplacian(adj_mx)\n",
    "    cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "    model = ASTGCN_submodule(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        else:\n",
    "            nn.init.uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
